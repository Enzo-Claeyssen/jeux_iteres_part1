Jour 1 :

- Prise de connaissance des sujets
- Reprise du code fourni
- Première comparaison Q-Learning / DQN selon taille du couloir
- Tentative RLlib (ratée on reste sur StableBaselines3)
- Compréhension environnement Gym
- Création de l'environnement frozenLake basé sur Gym
- Généralisation Q-Learning pour tout problème Gym



Jour 2 :

- Généralisation des méthodes test-q-learning et test-dqn en prévision de l'entraînement sur labyrinthe
- Séparation des problèmes par fiche
- Rédaction de base.ipynb pour une présentation sommaire
- Changement de l'unité d'itération de QLearning. (On passe de Épisode à TimeSteps pour avoir la même unité que DQN)
- Ajout de la possibilité d'obtenir les récompenses hors entraînement pour QLearning
- Optimisation des paramètres du QLearning pour problème du couloir



Jour 3 :

- Ajout logging des récompenses hors entraînement pour QLearning et DQN
- Retour sur optimisation du QLearning
- Optimisation de DQN
- Comparaison QLearning DQN par timesteps
- Comparaison QLearning DQN par temps
