{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39ea223f-f571-4f9d-9f09-62dc0cb5bfbf",
   "metadata": {},
   "source": [
    "## Objectifs\n",
    "\n",
    "\n",
    "1. Découvrir et apprendre à utiliser certaines librairies (Gym, Gymnasium, StableBaselines)\n",
    "\n",
    "2. Comprendre comment régler certains paramètres d'algorithmes d'apprentissage (Q-Learning, DQN et PPO) afin d'améliorer leurs performances sur certains problèmes.\n",
    "\n",
    "3. Comparer les résultats des divers algorithmes et interpréter les observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09479a30-e3ea-464b-b1ea-43fc1423f803",
   "metadata": {},
   "source": [
    "## Termes importants\n",
    "\n",
    "\n",
    "- **step** (ou timestep) : un cycle de trois phases ou l'agent : observe, agit sur l'environnement puis se voit attribuer une récompense immédiate. L'agent n'effectue qu'une unique action par step.\n",
    "\n",
    "- **épisode** : une succession de steps ammenant l'agent du début de l'épisode à la fin de l'épisode\n",
    "\n",
    "- **récompense immédiate** : Récompense donnée par l'environnement à l'agent lorsque celui-ci effectue une action\n",
    "\n",
    "- **récompense** (pour un épisode) : Somme des récompenses immédiates obtenues par l'agent au cours d'un épisode.\n",
    "\n",
    "\n",
    "\n",
    "Exemple :\n",
    "  Une partie d'échec est un épisode qui possède un début et une fin.\n",
    "  La partie correspond à l'épisode et le step correspond au fait de bouger un pion.\n",
    "\n",
    "\n",
    "- **État de l'environnement** : la configuration à l'instant t de l'environnement\n",
    "\n",
    "- **Observation de l'agent** : Ce que l'agent observe de son environnement à l'instant t\n",
    "\n",
    "Exemple :\n",
    "  Si je fais dos à un porte ouverte, alors dans l'état de mon environnement la porte est ouverte.\n",
    "  Mais je ne suis pas en mesure de la voir, donc la porte ne fait pas partie de mon observation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8eaf102-976e-4f10-a5c9-e69695bf99f1",
   "metadata": {},
   "source": [
    "## Les algorithmes\n",
    "\n",
    "On utilise le Q-Learning tabulaire, DQN et PPO dont on trouvera l'implémentation dans leur fichier python respectif : QLearning.py, DQN.py et PPO.py\n",
    "\n",
    "Les trois fichiers comportent chacun deux fonctions importantes : train-XXX et test-XXX ou on remplace XXX par l'algorithme à utiliser.\n",
    "\n",
    "- La fonction train permet d'entraîner un agent et d'en obtenir le modèle ainsi que la suite de récompenses obtenues au cours de l'entraînement\n",
    "\n",
    "- La fonction test permet de spécifier un environnement ainsi qu'un modèle et de dérouler un épisode\n",
    "\n",
    "\n",
    "\n",
    "Attention, l'entraînement se fait sur un certains nombre de timesteps fixe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0a1298-aa6a-4fff-b5d9-88f03749e449",
   "metadata": {},
   "source": [
    "### QLearning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a87780-b626-48b2-b367-1585f9520b0b",
   "metadata": {},
   "source": [
    "Le QLearning fonctionne à l'aide d'**une table** où **chaque case est associée à une paire (action, état)**.\n",
    "Sémantiquement, une case (associée à l'action a et l'état e) contient la récompense d'épisode que l'on peut espérer obtenir en effectuant l'action a en étant dans l'état e.\n",
    "\n",
    "Grâce à la table nous pouvons modifier chaque case individuellement. Lorsque l'agent apprend quelque chose, il modifie son \"intérêt\" envers une action uniquement pour un état.\n",
    "\n",
    "\n",
    "Le QLearning est \"**off-policy**\", il n'agit pas de la même façon en entraînement et hors entraînement.\n",
    "\n",
    "- **Hors entraînement**, il suit la policy \"**greedy**\" : choisir l'action menant à la plus grande récompense d'épisode (d'après la table).\n",
    "\n",
    "- **En entraînement**, il suit la policy \"**epsilon-greedy**\" : avec probabilité epsilon j'agis aléatoirement, sinon j'applique la policy greedy.\n",
    "\n",
    "epsilon est une valeure que nous pouvons influencer à l'aide des paramètres."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c7d051-f707-46b0-9f79-beb95a190fe8",
   "metadata": {},
   "source": [
    "#### Paramètres du QLearning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b383cbae-a6be-443b-b497-6e119cb79c88",
   "metadata": {},
   "source": [
    "**Paramètres liés à epsilon** : \n",
    "\n",
    "- **eps_start** : La valeure de epsilon en début d'entraînement\n",
    "- **eps_end** : la valeure finale de epsilon\n",
    "- **eps_fraction** : le \"pourcentage\" de l'entraînement nécessaire pour que epsilon atteigne eps_end.\n",
    "\n",
    "La décroissance de epsilon est linéaire.\n",
    "\n",
    "**Exemple** :\n",
    "\n",
    "Si nous avons eps_start = 1, eps_end = 0 et eps_fraction = 0.5 et que l'entraînement se fait sur 100 timesteps.\n",
    "\n",
    "Alors au bout de 100*0.5 = 500 timesteps, epsilon vaudra 0.\n",
    "Comme la décroissance de epsilon est linéaire alors, au timesteps 250 de l'entraînement la valeur de epsilon sera de 0.5.\n",
    "\n",
    "\n",
    "**Valeurs par défaut**\n",
    "- eps_start = 0.9\n",
    "- eps_end = 0.05\n",
    "- eps_fraction = 0.3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Autres paramètres**\n",
    "\n",
    "- **alpha** (aussi appelé learningRate) : Au plus alpha est élevé au plus ce qui vient d'être observé est important par rapport à ce qui a déjà été appris par l'agent.\n",
    "\n",
    "Pour le QLearning, alpha est constant.\n",
    "\n",
    "- **gamma** : Au plus gamma est élevé, au plus les récompenses immédiates dont l'agent anticipe l'obtention dans plusieurs timesteps futurs\n",
    "seront importantes.\n",
    "\n",
    "\n",
    "**Valeurs par défaut**\n",
    "- alpha = 0.1\n",
    "- gamma = 0.9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efaa8a7-ddff-4af3-80fd-e76a5315150d",
   "metadata": {},
   "source": [
    "### DQN (Deep Q Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224daf14-81e7-43a2-82f3-c4aa401feb78",
   "metadata": {},
   "source": [
    "Se base sur les mêmes principes que le QLearning.\n",
    "\n",
    "La table est remplacée par un réseau de neurones artificiel.\n",
    "\n",
    "L'**inconvénient** de ce réseau est que lorsque que l'agent apprend quelque chose, cela peut modifier son \"intérêt\" pour de multiples actions dans divers états. Cela rend l'apprentissage **plus instable**, pour palier à cela on utilise un second réseau appelé **réseau cible** (target network).\n",
    "\n",
    "Ce même phénomène est aussi un **avantage** puisqu'il peut apprendre de multiples choses simultanément ce qui peut **accélérer l'entraînement** et surtout le réseau nécessite **moins de mémoire** que la table pour des problèmes complexes.\n",
    "\n",
    "Tout comme le QLearning, DQN est off-policy et nécessite un epsilon (dont on utilise strictement de la même façon que pour QLearning)\n",
    "\n",
    "\n",
    "**Les réseaux**\n",
    "\n",
    "- À chaque step, les informations (état, action, récompense immédiate) sont préservées dans un **buffer**.\n",
    "- Puis un certains nombre d'exemples issus du buffer sont placés dans le **batch** que l'on utilise pour mettre à jour le réseau principal.\n",
    "- Au bout d'un certain nombre de timesteps, on copie le réseau principale pour en faire le réseau cible.\n",
    "- Des paramètres supplémentaires permettent d'influencer les réseaux."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803b543f-ae81-4cc7-a34f-82d1b47f869e",
   "metadata": {},
   "source": [
    "#### Paramètres de DQN (pour StableBaselines3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e2f910-da85-4807-a87c-e1d4e17595cf",
   "metadata": {},
   "source": [
    "**Paramètres liés à epsilon** : \n",
    "\n",
    "- **exploration_initial_eps** : La valeure de epsilon en début d'entraînement\n",
    "- **exploration_final_eps** : la valeure finale de epsilon\n",
    "- **exploration_fraction** : le \"pourcentage\" de l'entraînement nécessaire pour que epsilon atteigne eps_end.\n",
    "\n",
    "La décroissance de epsilon est linéaire.\n",
    "\n",
    "**Exemple** :\n",
    "\n",
    "Si nous avons exploration_initial_eps = 1, exploration_final_eps = 0 et exploration_fraction = 0.5 et que l'entraînement se fait sur 100 timesteps.\n",
    "\n",
    "Alors au bout de 100*0.5 = 500 timesteps, epsilon vaudra 0.\n",
    "Comme la décroissance de epsilon est linéaire alors, au timesteps 250 de l'entraînement la valeur de epsilon sera de 0.5.\n",
    "\n",
    "\n",
    "**Valeurs par défaut**\n",
    "- exploration_initial_eps = 1\n",
    "- exploration_final_eps = 0.05\n",
    "- exploration_fraction = 0.3\n",
    "\n",
    "\n",
    "\n",
    "**Paramètres liés aux réseaux** :\n",
    "- **net_arch** : L'architecture des réseaux\n",
    "- **buffer_size** : Le nombre de tuple conservés dans le buffer\n",
    "- **batch_size** : Le nombre de tuple sélectionnés pour l'entraînement\n",
    "- **update_freq** : Le nombre de timesteps avant de recopier le réseau sur le réseau cible.\n",
    "Au plus update_freq est grand, au plus l'entraînement est lent mais stable.\n",
    "Un update_freq de 1 revient à avoir un unique réseau.\n",
    "\n",
    "**Valeurs par défaut**\n",
    "- net_arch = [64, 64]\n",
    "- buffer_size = 500\n",
    "- batch_size = 32\n",
    "- update_freq = 500\n",
    "\n",
    "\n",
    "**Autres paramètres**\n",
    "\n",
    "- **learning_rate** (pour learningRate) : Au plus le learning rate est élevé au plus ce qui vient d'être observé est important par rapport à ce qui a déjà été appris par l'agent.\n",
    "\n",
    "- **gamma** : Au plus gamma est élevé, au plus les récompenses immédiates dont l'agent anticipe l'obtention dans plusieurs timesteps futurs\n",
    "seront importantes.\n",
    "\n",
    "\n",
    "\n",
    "**Valeurs par défaut**\n",
    "- learning_rate = 0.01\n",
    "- gamma = 0.99\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262da422-719f-42e6-a5c5-0b314929ab6e",
   "metadata": {},
   "source": [
    "### PPO : À FAIRE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947eedd6-4cf5-4869-b340-8c2d73bb2bf6",
   "metadata": {},
   "source": [
    "Attention les valeurs par défaut de stable baselines diffèrent selon l'algorithme utilisé."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269d2708-b2c5-436c-9219-b12e7a89398a",
   "metadata": {},
   "source": [
    "### Utilisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f707c3-8816-4030-a6ad-539c3fc5c5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Entraînement de 2000 steps d'un QLearning sur un environnement avec un alpha de 0.01\"\"\"\n",
    "# env = monEnvironnement(param1, param2)\n",
    "# QTable, QrewardsTrain, QrewardsExploitation = train_q_learning(env, timesteps = 2000, alpha = 0.01, useProdForReward = True)\n",
    "\"\"\"\n",
    "Les paramètres non spécifiés prennent la valeur par défaut\n",
    "- env : on spécifie l'environnement sur lequel l'agent s'entraîne\n",
    "- timesteps : le nombre de steps d'entraînement effectués\n",
    "- alpha : dans cet exemple on précise ce paramètre pour qu'il ne prenne pas la valeur par défaut.\n",
    "- useProdForReward : Si True alors à chaque fin d'épisode pendant l'entraînement, on effectue\n",
    "épisode d'exploitation duquel on prend la récompense totale obtenue\n",
    "Si False, alors ces épisodes ne sont pas calculés.\n",
    "\n",
    "\n",
    "3 choses sont obtenues par l'utilisation de cette méthode :\n",
    "- QTable : La table Q après entraînement\n",
    "- QrewardsTrain : la liste des récompenses d'épisodes obtenues pendant l'entraînement (1 valeur = 1 épisode)\n",
    "- QrewardsExploitation : lorsque useProdForReward vaut True (False par défaut), à chaque fin d'épisode pendant l'entraînement, on effectue\n",
    "épisode d'exploitation duquel on prend la récompense totale obtenue.\n",
    "QrewardsExploitation correspond à la liste des récompenses d'épisodes obtenues par ces épisodes d'exploitation\n",
    "\n",
    "À savoir que si useProdForReward vaut False alors ces épisodes d'exploitation ne sont pas calculés et l'apprentissage est plus rapide.\n",
    "\n",
    "\n",
    "Pour cet exmeple, on effectue un entraînement sur 2000 timesteps, si au step 2000 l'épisode en cours ne prends pas fin alors l'entraînement\n",
    "prend tout de même fin mais les récompenses de cet épisode ne sont pas inclus dans les valeurs de retour.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# DQNmodel, DQNrewTrain, DQNrewExpl = train_dqn(env, timesteps = 2000, buffer_size = 1000, useProdForReward = True, maxTimestepsProd = 20)\n",
    "\n",
    "\"\"\"\n",
    "Pour cet exemple, on entraîne un DQN pendant 2000 steps avec un buffer_size de 1000, les valeurs de retour et les paramètres ont la même\n",
    "sémantique que pour train_q_learning()\n",
    "\n",
    "Le paramètre maxTimestepsProd permet de définir le nombre de timesteps maximal qu'effectue l'agent au cours d'un épisode d'exploitation.\n",
    "Cela permet d'empêcher une boucle infinie pendant le calcul d'une récompense d'exploitation. Par défaut, maxTimestepsProd vaut 100\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdd6fbf-a3d9-41f7-85c0-4e9479c200a5",
   "metadata": {},
   "source": [
    "## Les environnements (Gym et Gymnasium)\n",
    "\n",
    "Les algorithmes utilisés fonctionnent avec tous les environnement \"Gym\" qui suivent le schéma suivant :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183b9d06-d2bf-4b5d-8cfd-47f9c202a04d",
   "metadata": {},
   "source": [
    "### Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73075169-6237-402c-9e81-22ae7c97fabf",
   "metadata": {},
   "source": [
    "La classe de l'environnement doit hériter de gym.Env\n",
    "\n",
    "\n",
    "Il faut les méthodes :\n",
    "\n",
    "- **__init__** : initialise l'environnement\n",
    "\n",
    "aucun paramètre et ne renvoie rien,\n",
    "\n",
    "Doit commencer par super(nomClasse, self).__init__()\n",
    "\n",
    "Doit définir :\n",
    "1. self.action_space : décrit les actions possibles\n",
    "2. self.observation_space : décrit les états possibles\n",
    "\n",
    "\n",
    "- **reset** : reinitialise l'environnement,\n",
    "\n",
    "ne prend aucun paramètre\n",
    "renvoie l'état de départ\n",
    "\n",
    "\n",
    "- **step** : applique une action à l'environnement,\n",
    "prend l'action en paramètre et renvoie\n",
    "\n",
    "1. Le nouvel état de l'environnement\n",
    "2. La récompense immédiate obtenue par l'agent\n",
    "3. True si l'épisode est fini, False sinon\n",
    "4. Un dictionnaire contenant des infos complémentaires\n",
    "\n",
    "\n",
    "- **seed** : initialise les générateurs pseudo-aléatoire\n",
    "\n",
    "Prend en paramètre la seed que doivent prendre ne paramètre les générateurs\n",
    "\n",
    "ne renvoie rien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b177464-c393-4a26-97b7-1be637d36b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class monEnvironnement(gym.Env) :\n",
    "\n",
    "    def __init__(self) :\n",
    "        super(monEnvironnement, self).__init__()\n",
    "        # J'initialise l'environnement\n",
    "        self.action_space = spaces.Discrete(2)       #OBLIGATOIRE\n",
    "        self.observation_space = spaces.Discrete(4)  #OBLIGATOIRE\n",
    "        pass\n",
    "\n",
    "    def reset(self) :\n",
    "        # Je réinitialise l'environnement\n",
    "        # Je renvoie l'état initial (int)\n",
    "        pass\n",
    "\n",
    "    def step(self, action) :\n",
    "        # J'applique l'action (int) effectué par l'agent dans l'environnement\n",
    "        # Je renvoie dans cet ordre :\n",
    "        # Le nouvel état de l'environnement (int)\n",
    "        # La récompense immédiate octroyée à l'agent (int)\n",
    "        # True si l'épisode est fini, False sinon (bool)\n",
    "        # Informations complémentaires (dict)\n",
    "        pass\n",
    "\n",
    "    def render(self) :\n",
    "        # Affiche l'état courant\n",
    "        # ne renvoie rien\n",
    "        # non obligatoire pour stableBaselines\n",
    "        pass\n",
    "\n",
    "    def seed(self, seedInt) :\n",
    "        # Prend l'entier comme seed\n",
    "        # N'est utile que pour les environnements non déterministe \n",
    "        # Permet d'affecter aux générateurs aléatoires cette seed précise\n",
    "        # Ne renvoie rien\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8889e9d6-030f-42fd-94e1-7540cbc862c4",
   "metadata": {},
   "source": [
    "dans la méthode init, deux attributs doivent être définis :\n",
    "- action_space : représente l'ensemble des actions possibles\n",
    "- observation_space : représente l'ensemble des états possibles\n",
    "\n",
    "Pour définir ces attributs on utilise spaces de gym."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda44472-ab3f-493f-b406-7a9ff35a805c",
   "metadata": {},
   "source": [
    "### Pour Gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1bc0fb-6d5c-4ffc-89e0-540a912b63ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "from gymnasium import spaces\n",
    "\n",
    "class monEnvironnement(gymnasium.Env) :\n",
    "\n",
    "    def __init__(self) :\n",
    "        super(monEnvironnement, self).__init__()\n",
    "        # J'initialise l'environnement\n",
    "        self.action_space = spaces.Discrete(2)       #OBLIGATOIRE\n",
    "        self.observation_space = spaces.Discrete(4)  #OBLIGATOIRE\n",
    "        pass\n",
    "\n",
    "    def reset(self, seed = None, options = None) :\n",
    "        super().reset(seed = seed)\n",
    "        # Je réinitialise l'environnement\n",
    "        # Je renvoie l'état initial (int) puis un dictionnaire d'info\n",
    "        pass\n",
    "\n",
    "    def step(self, action) :\n",
    "        # J'applique l'action (int) effectué par l'agent dans l'environnement\n",
    "        # Je renvoie dans cet ordre :\n",
    "        # Le nouvel état de l'environnement (int)\n",
    "        # La récompense immédiate octroyée à l'agent (int)\n",
    "        # True si l'épisode est fini, False sinon (bool)\n",
    "        # Informations complémentaires (dict)\n",
    "        pass\n",
    "\n",
    "    def render(self) :\n",
    "        # Affiche l'état courant\n",
    "        # ne renvoie rien\n",
    "        # non obligatoire pour stableBaselines\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57188f61-1c2b-4f69-9b0b-fc274f82ec4f",
   "metadata": {},
   "source": [
    "### Gym.spaces == Gymnasium.spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebbae7b-87ee-410c-bd05-808bacf61858",
   "metadata": {},
   "source": [
    "Pour le moment nos environnements feront usage de spaces.Discrete qui décrivent un ensemble discret.\n",
    "\n",
    "Exemple :\n",
    "- spaces.Discrete(2) correspond à l'ensemble {0, 1}\n",
    "- spaces.Discrete(3, start = -1) correspond à l'ensemble {-1, 0, 1}\n",
    "\n",
    "Si on définit action_space avec spaces.Discrete(2), alors 2 actions sont possibles, 0 ou 1.\n",
    "\n",
    "À noter que spaces s'utilise de la même manière entre Gym et Gymnasium il suffit d'utiliser celui correspondant à l'environnement que l'on crée (gym ou gymnasium)\n",
    "\n",
    "Il existe d'autres spaces pour des situations que nous ne rencontrerons pas dans les environnements étudiés sur ce dépôt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d1e8c7-1717-488f-9de7-c3ffaee964d0",
   "metadata": {},
   "source": [
    "## StableBaselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad21f702-aa11-4c45-997f-290d0a065f9a",
   "metadata": {},
   "source": [
    "Stable Baselines est une librairie fournissant une implémentation de divers algorithmes, nous l'utiliserons pour les algorithmes de DQN et de PPO.\n",
    "\n",
    "Voici des exemples montrant comment créer un modèle, l'entraîner, l'utiliser et l'évaluer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45be13c1-0a00-4a5a-8270-fd133083f83b",
   "metadata": {},
   "source": [
    "### Créer un modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f56db9c-b461-4b09-99f8-60559b8a477f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\"MlpPolicy\" permet d\\'utiliser un réseau dense\\nenv précise l\\'environnement\\npolicy_kwargs = policy_kwargs permet d\\'utiliser le réseau de notre choix\\n(par défaut, 2 couches intermédiaires de 64)\\nverbose = 0 : aucun affichage\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"On commence par spécifier l'architecture du réseau\"\"\"\n",
    "import torch as th\n",
    "\n",
    "policy_kwargs = dict(activation_fn=th.nn.ReLU,\n",
    "                     net_arch=[8, 10])\n",
    "# Chaque élément est une couche dont la valeur est le nombre de neurone qui s'y trouvent.\n",
    "# Cela forme les couches intermédiaires, dans cet exemple on a deux couches intermédiaires\n",
    "# de respectivement 8 et 10 neurones.\n",
    "\n",
    "\n",
    "env = monEnvironnement()\n",
    "\n",
    "\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "model = DQN(\"MlpPolicy\", env, policy_kwargs = policy_kwargs, verbose = 0)\n",
    "\n",
    "\"\"\"\n",
    "\"MlpPolicy\" permet d'utiliser un réseau densse\n",
    "env précise l'environnement\n",
    "policy_kwargs = policy_kwargs permet d'utiliser le réseau de notre choix\n",
    "(par défaut, 2 couches intermédiaires de 64)\n",
    "verbose = 0 : aucun affichage\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dfb791-70eb-4153-923a-ba1635c150a3",
   "metadata": {},
   "source": [
    "### Le logger : calculer les récompenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3c73d5a-0798-45a0-899c-e7b26b416467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nLe logger ci-dessus permet de calculer les récompenses d'épisodes au cours de l'entraînement.\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "On doit créer un logger pour enregistrer les récompenses.\n",
    "\"\"\"\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "\n",
    "class RewardLogger(BaseCallback) :\n",
    "    def __init__(self, verbose = 0) :\n",
    "        super().__init__(verbose)\n",
    "        self.rewards = []\n",
    "        self.current_reward = 0\n",
    "\n",
    "    def _on_step(self) :\n",
    "        self.current_reward += self.locals[\"rewards\"][0] # Permet d'obtenir la dernière récompense immédiate\n",
    "        if self.locals[\"dones\"][0] : # Permet de savoir si l'épisode est terminé\n",
    "            self.rewards += self.current_reward\n",
    "            self.current_reward = 0\n",
    "        return True\n",
    "\n",
    "\n",
    "logger = RewardLogger()\n",
    "\n",
    "\"\"\"\n",
    "Le logger ci-dessus permet de calculer les récompenses d'épisodes au cours de l'entraînement.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a293c546-3343-4ea6-a28a-5b391edaf4e6",
   "metadata": {},
   "source": [
    "### Entraînement et affichage des récompenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b60c9ef4-71ac-4342-bd8f-eafe687c77f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "On effectue l'entraînement puis on affiche les résultats\n",
    "La partie ci-dessous est commentée car nous n'avons pas d'environnement donc cela ne fonctionnerai pas.\n",
    "\"\"\"\n",
    "\n",
    "# model.learn(total_timesteps = NombreDeTimesteps, callback = logger)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.plot(logger.rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf47278-1db4-49ca-8cfd-598d3cfcd7ac",
   "metadata": {},
   "source": [
    "### Dérouler un épisode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfb5139-dfbc-48ac-8fde-99162395fa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut observer ce qu'effectue l'agent dans un environnement\n",
    "\n",
    "\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# env = ....\n",
    "\n",
    "\"\"\"\n",
    "preventInfinite = nombreMaxDeStep\n",
    "done = False\n",
    "\n",
    "env = make_vec_env(lambda : env, n_env = 1)          Stable Baselines utilise des \"environnements vectorisés\" \n",
    "obs = env.reset()                               Ne pas oublier de réinitialiser l'environnement si on boucle sur ce script.\n",
    "\n",
    "while not done && preventInfinite > 0 :\n",
    "    preventInfinite -= 1\n",
    "    action, _ = model.predict(obs, deterministic = True)    deterministic = True est nécessaire pour éviter d'utiliser les policy d'entraînement\n",
    "    obs, reward, done, info = env.step(action)             Après avoir déterminé l'action de l'agent, on l'applique à l'environnement\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4523b9-d599-40e4-b1d0-fd20b86573d6",
   "metadata": {},
   "source": [
    "Tout cela est implémenté dans les fichiers DQN.py, il suffit d'utiliser train_dqn() et test_dqn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0701bbaa-87f8-4665-a028-d69dc6b4babb",
   "metadata": {},
   "source": [
    "## Les différents problèmes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d33fa9c-90bd-4cae-a40a-516ac8488635",
   "metadata": {},
   "source": [
    "### Problème 1 : Le couloir\n",
    "\n",
    "La fiche couloir.ipynb traite ce problème.\n",
    "Ce problème est assez simple, il a pour objectif de prendre en main les différentes librairies et d'établir une méthode à suivre pour l'étude des prochains problèmes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d81620-b49a-491f-97fc-932c85230ab0",
   "metadata": {},
   "source": [
    "Un couloir peut être imaginé comme une série de cases les unes à côtés des autres.\n",
    "\n",
    "La longueur du couloir correspond au nombre de cases qui le représentent\n",
    "\n",
    "L'agent ne perçoit qu'un entier, celui correspondant à la case où il se trouve.\n",
    "S'il se trouve sur la première case alors il observe 0, il observera 1 s'il se trouve sur la seconde case etc..\n",
    "\n",
    "Soit n la position actuelle de l'agent.\n",
    "\n",
    "À chaque pas, l'agent peut effectuer deux actions, aller à gauche (se déplacer en n-1 pour n non nul) ou aller à droite (se déplacer en n+1).\n",
    "L'épisode se termine lorsque l'agent se trouve en (taille du couloir - 1)\n",
    "\n",
    "\n",
    "Les récompenses sont distribuées ainsi :\n",
    "- Si l'agent n'est pas en taille - 1 : -1\n",
    "- Si l'agent se trouve en taille - 1 : 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ed306f-2070-4bda-abf1-03154ac40206",
   "metadata": {},
   "source": [
    "### Problème 2 : Le Labyrinthe\n",
    "\n",
    "La fiche Labyrinthe.ipynb traite ce problème.\n",
    "\n",
    "Ce problème introduit d'avantages d'actions et d'états possibles. Il est basé sur l'environnement FrozenLake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f303676-afbb-40a0-9518-8d144d7da012",
   "metadata": {},
   "source": [
    "L'objectif est d'atteindre la sortie d'un labyrinthe.\n",
    "Le labyrinthe est représenté par une succession de cases formant un carré dont on peut choisir la taille.\n",
    "\n",
    "À chaque case est associé un numéro qui représente sa position dans le labyrinthe.\n",
    "L'agent ne peut perçevoir que le numéro associé à sa position actuelle.\n",
    "\n",
    "L'agent résoud les labyrinthes un par un (il s'entraîne sur un unique labyrinthe pour en trouver la solution).\n",
    "\n",
    "\n",
    "\n",
    "À chaque pas, l'agent peut :\n",
    "- aller en haut\n",
    "- aller en bas\n",
    "- aller à gauche\n",
    "- aller à droite\n",
    "\n",
    "Mais il n'est pas possible de sortir des limites du labyrinthe (aller en haut en étant tout en haut n'a donc aucun effet)\n",
    "\n",
    "\n",
    "Le labyrinthe possède de multiples case :\n",
    "- \"F\" : une case libre\n",
    "- \"G\" : la sortie du labyrinthe\n",
    "- \"S\" : le point de départ de l'agent\n",
    "- \".\" : la position actuelle de l'agent\n",
    "- \"H\" : un trou\n",
    "\n",
    "  Dans ce labyrinthe, les murs sont remplacés par des trous.\n",
    "\n",
    "L'épisode prend fin si :\n",
    "- l'agent tombe dans un trou\n",
    "- l'agent trouve la sortie\n",
    "\n",
    "La \"taille\" dy labyrinthe fait référence à la longueur du côté du carré le représentant. Il possède en tout taille^2 cases.\n",
    "\n",
    "Le point de départ de l'agent est toujours en haut à gauche et la sortie toujours en bas à droite.\n",
    "Quelque soit le labyrinthe, il existe au moins une solution.\n",
    "\n",
    "\n",
    "Les récompenses sont distribuées de cette façon :\n",
    "- tomber dans un trou : -1\n",
    "- arriver sur une case libre : -1 / (taille ^ 2)\n",
    "- trouver la sortie : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ec0728-ba21-4e9a-8fa6-9dd59b5804d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
